{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usCvd9-Pv848"
   },
   "source": [
    "# Model training with image training dataset\n",
    "\n",
    "Owner: Ling-Yun, Huang\n",
    "\n",
    "Project Title: Mask-wear Detection using Computer Vision Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKCji7bUcCD2"
   },
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11824,
     "status": "ok",
     "timestamp": 1713634186494,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "i95zcHHNbzws"
   },
   "outputs": [],
   "source": [
    "# for accessing and preprocessing the dataset\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# HOG\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "\n",
    "# SIFT\n",
    "from skimage import img_as_ubyte, io, color\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# SVM\n",
    "from sklearn import svm, metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Precision and F-score are ill-defined*\")\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "# MLPs\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# CNNs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchvision import transforms\n",
    "torch.cuda.manual_seed(46)\n",
    "torch.manual_seed(46)\n",
    "\n",
    "# Model Performance and images output\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AXaDZ0au3Sy"
   },
   "source": [
    "# Access the provided dataset\n",
    "\n",
    "Two dataset will be loading in this section. One is from provided datasets, another is video from internet.\n",
    "\n",
    "*Unzipped dataset adapted from Lab 6 code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1789,
     "status": "ok",
     "timestamp": 1713634188268,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "vvPiiNc9uFUF"
   },
   "outputs": [],
   "source": [
    "# Unzipped dataset\n",
    "# the path of zip file\n",
    "zip_path = os.path.join(GOOGLE_DRIVE_PATH, 'CW_Dataset/CV2024_CW_Dataset.zip')\n",
    "\n",
    "# Copy it to Colab\n",
    "!cp '{zip_path}' .\n",
    "\n",
    "# Unzip it\n",
    "!yes|unzip -q CV2024_CW_Dataset.zip\n",
    "\n",
    "# Delete zipped version from Colab\n",
    "!rm CV2024_CW_Dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HoJzKE7Eymt"
   },
   "source": [
    "Functions to read and preprocess images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713634188268,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "89h58raYuFW9"
   },
   "outputs": [],
   "source": [
    "# Functions to read images and labels stored as dictionary with filename\n",
    "def read_labels(path):\n",
    "    labels = {}\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(path, file), 'r') as f:\n",
    "                labels[file.split('.')[0]] = f.read()\n",
    "    return labels\n",
    "\n",
    "def read_images(path):\n",
    "    images = {}\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.jpeg'):\n",
    "            img = cv2.imread(os.path.join(path, file))\n",
    "            images[file.split('.')[0]] = img\n",
    "    return images\n",
    "\n",
    "# Function to preporcess images\n",
    "def preprocess_images(images):\n",
    "    processed_images = []\n",
    "    for image in images:\n",
    "        resized_image = cv2.resize(image, (32, 32)) # Resize image to 32x32\n",
    "        normalized_image = resized_image.astype('float32') / 255.0 # Convert image to float32 and normalize to range [0, 1]\n",
    "        processed_images.append(normalized_image)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd2UQ3VLqAke"
   },
   "source": [
    "Loading provided dataset in train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1713634188606,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "PpZDpvrRuFcq",
    "outputId": "d123ee91-f2de-4f5c-d07e-445b538d21cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'1': 1940, '0': 376, '2': 78})\n"
     ]
    }
   ],
   "source": [
    "# filepath for train dataset\n",
    "image_train_dir = 'train/images'\n",
    "label_train_dir = 'train/labels'\n",
    "\n",
    "# Read train labels and images\n",
    "labels_train = read_labels(label_train_dir)\n",
    "images_train = read_images(image_train_dir)\n",
    "\n",
    "# Store two lists for images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for filename, label in labels_train.items():\n",
    "    if filename in images_train:\n",
    "        train_images.append(images_train[filename])\n",
    "        train_labels.append(label)\n",
    "\n",
    "# Preprocess images\n",
    "train_images_transform = preprocess_images(train_images)\n",
    "\n",
    "print(Counter(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZRAvYRldroG"
   },
   "source": [
    "Here, we split train data into training and validation subset with 80/20 percentage. Since dataset is unbalanced itself, we also use stratify to make sure split equally.\n",
    "\n",
    "*Adopt from Lab 6 code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1713634188834,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "LnxM-U2Ddq9j",
    "outputId": "76f318b2-4025-45ad-fea9-7c14695566b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915 479\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_images_transform, train_labels, test_size=0.2,\n",
    "    shuffle=True, stratify=train_labels, random_state=46)\n",
    "\n",
    "print(len(X_train), len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1xK2KPG0nE2"
   },
   "source": [
    "#  Implement a series of image classification models to perform FCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji8JXzGG77CH"
   },
   "source": [
    "## HOG + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iueSKntklSYX"
   },
   "source": [
    "Extract HOG features\n",
    "\n",
    "*This part is adopted from Lab 5 code and [scikit-image library](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.hog).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48RAwCkwrIfq"
   },
   "outputs": [],
   "source": [
    "def HOG_extract(images):\n",
    "\n",
    "    # Extract HOG features from images\n",
    "    images_hog = []\n",
    "    for image in images:\n",
    "        features, hog_image = hog(image, orientations=8, pixels_per_cell=(8, 8),\n",
    "                        cells_per_block=(1, 1), visualize=True, channel_axis=2)\n",
    "        hog_image = exposure.equalize_hist(hog_image)\n",
    "        images_hog.append(features)\n",
    "\n",
    "    # Flatten the HOG features\n",
    "    images_hog = np.array(images_hog)\n",
    "\n",
    "    return images_hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d8jYJ-Br2Es"
   },
   "outputs": [],
   "source": [
    "# Extract HOG features for training data\n",
    "X_train_hog = HOG_extract(X_train)\n",
    "\n",
    "# Extract HOG features for validation data\n",
    "X_val_hog = HOG_extract(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ5mJJi3jWGf"
   },
   "source": [
    "Experienced different SVM classifier with different C and evaluate its performance.\n",
    "\n",
    "*The code is adopted from Lab 6 and [sklearn.svm.SVC library](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJoP57C4o8e4"
   },
   "outputs": [],
   "source": [
    "# Function of SVM classifier\n",
    "def SVM_classifier(kernel, C_value, X_train, y_train):\n",
    "    # Train SVM classifier\n",
    "    svm_classifier = svm.SVC(kernel=kernel, C = C_value, random_state=46)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    return svm_classifier\n",
    "\n",
    "# Evaluate with trained classifier\n",
    "def classifier_evaluate(classifier, X_test, y_test):\n",
    "    # Predict on validation data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    weighted_f1 = metrics.precision_recall_fscore_support(y_test, y_pred, average='weighted')[2]\n",
    "    report = metrics.classification_report(y_test, y_pred)\n",
    "\n",
    "    # print results\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Weighted F1-score:\", weighted_f1)\n",
    "    print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2685,
     "status": "ok",
     "timestamp": 1713513852103,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "PQnxXfD3q3w4",
    "outputId": "e91f18cc-76db-4272-8e5f-e6fee2336323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG + SVM with kernel = 'rbf', and C value = 0.01\n",
      "Accuracy: 0.8100208768267223\n",
      "Weighted F1-score: 0.7250013845646327\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        75\n",
      "           1       0.81      1.00      0.90       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.81       479\n",
      "   macro avg       0.27      0.33      0.30       479\n",
      "weighted avg       0.66      0.81      0.73       479\n",
      "\n",
      "HOG + SVM with kernel = 'rbf', and C value = 0.1\n",
      "Accuracy: 0.8100208768267223\n",
      "Weighted F1-score: 0.7250013845646327\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        75\n",
      "           1       0.81      1.00      0.90       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.81       479\n",
      "   macro avg       0.27      0.33      0.30       479\n",
      "weighted avg       0.66      0.81      0.73       479\n",
      "\n",
      "HOG + SVM with kernel = 'rbf', and C value = 1\n",
      "Accuracy: 0.8622129436325678\n",
      "Weighted F1-score: 0.8390633083395455\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.49      0.57        75\n",
      "           1       0.88      0.97      0.92       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.86       479\n",
      "   macro avg       0.52      0.49      0.50       479\n",
      "weighted avg       0.82      0.86      0.84       479\n",
      "\n",
      "HOG + SVM with kernel = 'rbf', and C value = 10\n",
      "Accuracy: 0.8643006263048016\n",
      "Weighted F1-score: 0.8522105315707275\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.61      0.63        75\n",
      "           1       0.91      0.95      0.93       388\n",
      "           2       0.33      0.06      0.11        16\n",
      "\n",
      "    accuracy                           0.86       479\n",
      "   macro avg       0.63      0.54      0.55       479\n",
      "weighted avg       0.85      0.86      0.85       479\n",
      "\n",
      "HOG + SVM with kernel = 'poly', and C value = 0.01\n",
      "Accuracy: 0.8455114822546973\n",
      "Weighted F1-score: 0.8116059525905589\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.33      0.45        75\n",
      "           1       0.86      0.98      0.92       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.85       479\n",
      "   macro avg       0.51      0.44      0.45       479\n",
      "weighted avg       0.80      0.85      0.81       479\n",
      "\n",
      "HOG + SVM with kernel = 'poly', and C value = 0.1\n",
      "Accuracy: 0.8517745302713987\n",
      "Weighted F1-score: 0.8403035458374636\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58        75\n",
      "           1       0.91      0.94      0.92       388\n",
      "           2       0.50      0.06      0.11        16\n",
      "\n",
      "    accuracy                           0.85       479\n",
      "   macro avg       0.66      0.53      0.54       479\n",
      "weighted avg       0.84      0.85      0.84       479\n",
      "\n",
      "HOG + SVM with kernel = 'poly', and C value = 1\n",
      "Accuracy: 0.8517745302713987\n",
      "Weighted F1-score: 0.8448102426650734\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.65      0.62        75\n",
      "           1       0.92      0.92      0.92       388\n",
      "           2       0.20      0.06      0.10        16\n",
      "\n",
      "    accuracy                           0.85       479\n",
      "   macro avg       0.57      0.55      0.54       479\n",
      "weighted avg       0.84      0.85      0.84       479\n",
      "\n",
      "HOG + SVM with kernel = 'poly', and C value = 10\n",
      "Accuracy: 0.8517745302713987\n",
      "Weighted F1-score: 0.8448102426650734\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.65      0.62        75\n",
      "           1       0.92      0.92      0.92       388\n",
      "           2       0.20      0.06      0.10        16\n",
      "\n",
      "    accuracy                           0.85       479\n",
      "   macro avg       0.57      0.55      0.54       479\n",
      "weighted avg       0.84      0.85      0.84       479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SVM classifier {‘rbf’, ‘poly’} with C_value {0.01, 0.1, 1, 10}\n",
    "\n",
    "svm_classifier_rbf_001 = SVM_classifier('rbf', 0.01, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'rbf', and C value = 0.01\")\n",
    "classifier_evaluate(svm_classifier_rbf_001, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_rbf_01 = SVM_classifier('rbf', 0.1, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'rbf', and C value = 0.1\")\n",
    "classifier_evaluate(svm_classifier_rbf_01, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_rbf_1 = SVM_classifier('rbf', 1, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'rbf', and C value = 1\")\n",
    "classifier_evaluate(svm_classifier_rbf_1, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_rbf_10 = SVM_classifier('rbf', 10, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'rbf', and C value = 10\")\n",
    "classifier_evaluate(svm_classifier_rbf_10, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_poly_001 = SVM_classifier('poly', 0.01, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'poly', and C value = 0.01\")\n",
    "classifier_evaluate(svm_classifier_poly_001, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_poly_01 = SVM_classifier('poly', 0.1, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'poly', and C value = 0.1\")\n",
    "classifier_evaluate(svm_classifier_poly_01, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_poly_1 = SVM_classifier('poly', 1, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'poly', and C value = 1\")\n",
    "classifier_evaluate(svm_classifier_poly_1, X_val_hog, y_val)\n",
    "\n",
    "svm_classifier_poly_10 = SVM_classifier('poly', 10, X_train_hog, y_train)\n",
    "print(\"HOG + SVM with kernel = 'poly', and C value = 10\")\n",
    "classifier_evaluate(svm_classifier_poly_10, X_val_hog, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg5kROeqlH3G"
   },
   "source": [
    "With highest weighted F1-score as the selected model in this combination.\n",
    "\n",
    "HOG + SVM with kernel = 'rbf' and C value = 10 is choosen. Now store the classifier for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1713513868920,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "fg8g2q1Wlm5U",
    "outputId": "27b55e29-065e-415d-aa80-210e66167c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, random_state=46)\n"
     ]
    }
   ],
   "source": [
    "HOGwSVM_classifier = svm_classifier_rbf_10\n",
    "\n",
    "# Store the classifier\n",
    "dump(HOGwSVM_classifier, os.path.join(GOOGLE_DRIVE_PATH, 'Models/HOGwSVM_classifier.joblib'))\n",
    "\n",
    "print(HOGwSVM_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x6fBZF37-gb"
   },
   "source": [
    "## SIFT + MLP\n",
    "\n",
    "*This section is adopted from Lab 6 code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGSZn8dbaULY"
   },
   "source": [
    "Using SIFT and apply k-means to create 30 groups as input neurones in MLPs\n",
    "\n",
    "Extract descriptors with SIFT in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcCc4gY38DVe"
   },
   "outputs": [],
   "source": [
    "# SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# lists for feature descriptors and labels\n",
    "des_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    # Identify keypoints and extract descriptors with SIFT\n",
    "    img = img_as_ubyte(color.rgb2gray(X_train[i]))\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "\n",
    "    # Append list of descriptors and label into lists\n",
    "    if des is not None:\n",
    "        des_list.append(des)\n",
    "        y_train_list.append(y_train[i])\n",
    "\n",
    "# Convert to array\n",
    "des_array = np.vstack(des_list)\n",
    "\n",
    "# Number of codewords\n",
    "k = 30\n",
    "\n",
    "# Use MiniBatchKMeans for clustering\n",
    "batch_size = des_array.shape[0] // 4\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, n_init='auto', random_state=46).fit(des_array)\n",
    "\n",
    "# Convert descriptors into histograms of codewords for each image\n",
    "hist_list = []\n",
    "idx_list = []\n",
    "\n",
    "for des in des_list:\n",
    "    hist = np.zeros(k)\n",
    "\n",
    "    idx = kmeans.predict(des)\n",
    "    idx_list.append(idx)\n",
    "    for j in idx:\n",
    "        hist[j] = hist[j] + (1 / len(des))\n",
    "    hist_list.append(hist)\n",
    "\n",
    "hist_array = np.vstack(hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713513879063,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "vvUcLbaIoiWJ",
    "outputId": "0dc44633-04a7-4a49-da34-266a761b6cc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drive/My Drive/CV2024-CW-230048952-LYHuang/Models/kmeans_SIFT.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store kmeans model\n",
    "dump(kmeans, os.path.join(GOOGLE_DRIVE_PATH, 'Models/kmeans_SIFT.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu1c9LceiujE"
   },
   "source": [
    "Extract descriptors with SIFT in validation set and assign into related kmeans group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMaCL0UTxWoL"
   },
   "outputs": [],
   "source": [
    "def SIFT_feature(images, labels, kmeans):\n",
    "    hist_list = []\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        img = img_as_ubyte(color.rgb2gray(images[i]))\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "\n",
    "        if des is not None:\n",
    "            hist = np.zeros(30)\n",
    "            idx = kmeans.predict(des)\n",
    "\n",
    "            for j in idx:\n",
    "                hist[j] = hist[j] + (1 / len(des))\n",
    "\n",
    "            hist_list.append(hist)\n",
    "\n",
    "        else:\n",
    "            hist_list.append(None)\n",
    "\n",
    "    # Remove images with no descriptors\n",
    "    idx_not_empty = [i for i, x in enumerate(hist_list) if x is not None]\n",
    "    hist_list = [hist_list[i] for i in idx_not_empty]\n",
    "    labels_list = [labels[i] for i in idx_not_empty]\n",
    "    hist_array = np.vstack(hist_list)\n",
    "\n",
    "    return hist_array, labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rr4HnSS1yRQG"
   },
   "outputs": [],
   "source": [
    "# Extract SIFT and assign into related kmeans groups on validation set\n",
    "hist_array_val, y_val_list = SIFT_feature(X_val, y_val, kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNb_ShsKlEe5"
   },
   "source": [
    "Training MLPs.\n",
    "\n",
    "*This part is adopted from [sklearn.neural_network.MLPClassifier library](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btWVy3sFq-5c"
   },
   "outputs": [],
   "source": [
    "# Function for MLP classifier\n",
    "def MLP_Classifier(X_train, y_train, hidden_size1, hidden_size2=None, verbose=False):\n",
    "\n",
    "    if hidden_size2 == None:\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(hidden_size1,), max_iter=500, alpha=1e-4,\n",
    "                      solver='sgd', verbose=verbose, random_state=46, learning_rate_init=.1)\n",
    "\n",
    "    else:\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(hidden_size1, hidden_size2), max_iter=500, alpha=1e-4,\n",
    "                      solver='sgd', verbose=verbose, random_state=46, learning_rate_init=.1)\n",
    "\n",
    "    # Fit classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18818,
     "status": "ok",
     "timestamp": 1713513974835,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "7AFtI3_krvIs",
    "outputId": "9c7ae04e-bfa0-4948-9074-0bb6ee894a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT + MLP with one hidden layer and 15 neurones\n",
      "Accuracy: 0.8138075313807531\n",
      "Weighted F1-score: 0.7808080361703448\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.27      0.34        74\n",
      "           1       0.85      0.95      0.90       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.81       478\n",
      "   macro avg       0.44      0.41      0.41       478\n",
      "weighted avg       0.76      0.81      0.78       478\n",
      "\n",
      "SIFT + MLP with one hidden layer and 30 neurones\n",
      "Accuracy: 0.8117154811715481\n",
      "Weighted F1-score: 0.7844697928379937\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.31      0.37        74\n",
      "           1       0.86      0.94      0.90       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.81       478\n",
      "   macro avg       0.43      0.42      0.42       478\n",
      "weighted avg       0.76      0.81      0.78       478\n",
      "\n",
      "SIFT + MLP with one hidden layer and 60 neurones\n",
      "Accuracy: 0.805439330543933\n",
      "Weighted F1-score: 0.7751495058287107\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.27      0.33        74\n",
      "           1       0.85      0.94      0.89       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.81       478\n",
      "   macro avg       0.42      0.40      0.41       478\n",
      "weighted avg       0.75      0.81      0.78       478\n",
      "\n",
      "SIFT + MLP with one hidden layer and 120 neurones\n",
      "Accuracy: 0.803347280334728\n",
      "Weighted F1-score: 0.783198525690477\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.34      0.36        74\n",
      "           1       0.87      0.93      0.90       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.80       478\n",
      "   macro avg       0.42      0.42      0.42       478\n",
      "weighted avg       0.77      0.80      0.78       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experience with one hidden layer and different neurone sizes in MLP classifier\n",
    "MLP_classifier_1_15 = MLP_Classifier(hist_array, y_train_list, 15)\n",
    "print(\"SIFT + MLP with one hidden layer and 15 neurones\")\n",
    "classifier_evaluate(MLP_classifier_1_15, hist_array_val, y_val_list)\n",
    "\n",
    "MLP_classifier_1_30 = MLP_Classifier(hist_array, y_train_list, 30)\n",
    "print(\"SIFT + MLP with one hidden layer and 30 neurones\")\n",
    "classifier_evaluate(MLP_classifier_1_30, hist_array_val, y_val_list)\n",
    "\n",
    "MLP_classifier_1_60 = MLP_Classifier(hist_array, y_train_list, 60)\n",
    "print(\"SIFT + MLP with one hidden layer and 60 neurones\")\n",
    "classifier_evaluate(MLP_classifier_1_60, hist_array_val, y_val_list)\n",
    "\n",
    "MLP_classifier_1_120 = MLP_Classifier(hist_array, y_train_list, 120)\n",
    "print(\"SIFT + MLP with one hidden layer and 120 neurones\")\n",
    "classifier_evaluate(MLP_classifier_1_120, hist_array_val, y_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10640,
     "status": "ok",
     "timestamp": 1713513985472,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "7YoSZb_s_Cs6",
    "outputId": "4d339b6f-4145-4f96-e81a-9521255568d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT + MLP with two hidden layer and 15 neurones\n",
      "Accuracy: 0.8075313807531381\n",
      "Weighted F1-score: 0.7935236481118585\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.45      0.44        74\n",
      "           1       0.88      0.91      0.89       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.81       478\n",
      "   macro avg       0.44      0.45      0.44       478\n",
      "weighted avg       0.78      0.81      0.79       478\n",
      "\n",
      "SIFT + MLP with two hidden layer and 30 neurones\n",
      "Accuracy: 0.797071129707113\n",
      "Weighted F1-score: 0.7832212276892113\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.41      0.40        74\n",
      "           1       0.87      0.90      0.89       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.80       478\n",
      "   macro avg       0.42      0.44      0.43       478\n",
      "weighted avg       0.77      0.80      0.78       478\n",
      "\n",
      "SIFT + MLP with two hidden layer and 60 neurones\n",
      "Accuracy: 0.801255230125523\n",
      "Weighted F1-score: 0.7763331038865403\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.30      0.34        74\n",
      "           1       0.86      0.93      0.89       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.80       478\n",
      "   macro avg       0.42      0.41      0.41       478\n",
      "weighted avg       0.76      0.80      0.78       478\n",
      "\n",
      "SIFT + MLP with two hidden layer and 120 neurones\n",
      "Accuracy: 0.7866108786610879\n",
      "Weighted F1-score: 0.7749563456229273\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.41      0.39        74\n",
      "           1       0.87      0.89      0.88       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.79       478\n",
      "   macro avg       0.41      0.43      0.42       478\n",
      "weighted avg       0.76      0.79      0.77       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experience with two hidden layers and different neurone sizes in MLP classifier\n",
    "MLP_classifier_2_15 = MLP_Classifier(hist_array, y_train_list, 15, 15)\n",
    "print(\"SIFT + MLP with two hidden layer and 15 neurones\")\n",
    "classifier_evaluate(MLP_classifier_2_15, hist_array_val, y_val_list)\n",
    "\n",
    "MLP_classifier_2_30 = MLP_Classifier(hist_array, y_train_list, 30, 30)\n",
    "print(\"SIFT + MLP with two hidden layer and 30 neurones\")\n",
    "classifier_evaluate(MLP_classifier_2_30, hist_array_val, y_val_list)\n",
    "\n",
    "MLP_classifier_2_60 = MLP_Classifier(hist_array, y_train_list, 60, 60)\n",
    "print(\"SIFT + MLP with two hidden layer and 60 neurones\")\n",
    "classifier_evaluate(MLP_classifier_2_60, hist_array_val, y_val_list)\n",
    "\n",
    "MLP_classifier_2_120 = MLP_Classifier(hist_array, y_train_list, 120, 120)\n",
    "print(\"SIFT + MLP with two hidden layer and 120 neurones\")\n",
    "classifier_evaluate(MLP_classifier_2_120, hist_array_val, y_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3pK63pV6GQM"
   },
   "source": [
    "With highest weighted F1-score as the selected model in this combination.\n",
    "\n",
    "SIFT + MLP with two hidden layers and 15 neurones is choosen. Now store the classifier for the model test part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1713514004476,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "5mvj0Dgd6W4z",
    "outputId": "e09d84be-1208-4b54-c39e-0c6deebfb007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(hidden_layer_sizes=(15, 15), learning_rate_init=0.1, max_iter=500,\n",
      "              random_state=46, solver='sgd')\n"
     ]
    }
   ],
   "source": [
    "SIFTwMLP_classifier = MLP_classifier_2_15\n",
    "\n",
    "# Store the trained classifier\n",
    "dump(SIFTwMLP_classifier, os.path.join(GOOGLE_DRIVE_PATH, 'Models/SIFTwMLP_classifier.joblib'))\n",
    "\n",
    "print(SIFTwMLP_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLEhNBcM00GK"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASIKdhII2k31"
   },
   "source": [
    "Convert training and validation dataset into tensor form\n",
    "\n",
    "*The coding is adopted from [pytorch tutorial datasets part](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1713634197314,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "zuLAovE-9nHd"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def data_tensor(images, labels):\n",
    "    data_means = [0.485, 0.456, 0.406]\n",
    "    data_stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # transform into tensor and normalize with means and stds above\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=data_means, std=data_stds)\n",
    "    ])\n",
    "\n",
    "    # convert str into int for labels\n",
    "    label_int = [int(label) for label in labels]\n",
    "\n",
    "    # Convert label_int into tensors\n",
    "    label_tensor = torch.tensor(label_int, dtype=torch.long)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = CustomDataset(images, label_tensor, transform=transform)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713634199451,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "Aj4fVtcffxPf"
   },
   "outputs": [],
   "source": [
    "# create train and val datasets\n",
    "train_dataset = data_tensor(X_train, y_train)\n",
    "val_dataset = data_tensor(X_val, y_val)\n",
    "\n",
    "# random split 90/10 percent in train_dataset to training/monitoring set to avoid overfitting\n",
    "monitoring_size = int(0.1 * len(train_dataset))\n",
    "training_size = len(train_dataset) - monitoring_size\n",
    "\n",
    "training_subset, monitoring_subset = random_split(train_dataset, [training_size, monitoring_size])\n",
    "\n",
    "# create dataloader in training_subse, monitoring_subset, val_dataset\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(training_subset, batch_size=batch_size, shuffle=True)\n",
    "monitor_loader = DataLoader(monitoring_subset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpo-J_wS2tnC"
   },
   "source": [
    "*The coding related CNNs training and evaluating are adopted from Lab 7 and [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html).*\n",
    "\n",
    "Defining CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1713634201436,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "LqJSbqnfqkhX"
   },
   "outputs": [],
   "source": [
    "## CNNs architectures\n",
    "# CNNs with one convolutional layer\n",
    "class CNN_1(nn.Module):\n",
    "    def __init__(self, hidden_size, kernel_size):\n",
    "        super(CNN_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=hidden_size,\n",
    "                               kernel_size=kernel_size, stride=1, padding=int((kernel_size-1)/2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(hidden_size * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# CNNs with two conventional layers\n",
    "class CNN_2(nn.Module):\n",
    "    def __init__(self, hidden_size, kernel_size):\n",
    "        super(CNN_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=hidden_size,\n",
    "                               kernel_size=kernel_size, stride=1, padding=int((kernel_size-1)/2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size,\n",
    "                               kernel_size=kernel_size, stride=1, padding=int((kernel_size-1)/2))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(hidden_size * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1713634203931,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "f-U6JYI1VJwe"
   },
   "outputs": [],
   "source": [
    "def CNN_model(layer, hidden_size, kernel_size, train_loader, monitor_loader):\n",
    "\n",
    "    torch.manual_seed(46) # set the random seed\n",
    "\n",
    "    # Decide the convolutional layer\n",
    "    if layer == 1:\n",
    "        model = CNN_1(hidden_size, kernel_size)\n",
    "    elif layer == 2:\n",
    "        model = CNN_2(hidden_size, kernel_size)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    fail_max = 5  # Number of epochs to wait if no improvement\n",
    "    min_loss = float('inf')  # Initialize minimum loss\n",
    "    fail_counter = 0  # Counter the epochs without improvement\n",
    "\n",
    "    # Train the CNNs\n",
    "    num_epochs = 25\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Training network\n",
    "        running_train_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "\n",
    "        model.eval() # evaluate with monitor set\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(monitor_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        avg_val_loss = running_val_loss / len(monitor_loader)\n",
    "\n",
    "        # print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < min_loss:\n",
    "            min_loss = avg_val_loss\n",
    "            fail_counter = 0\n",
    "        else:\n",
    "            fail_counter += 1\n",
    "            if fail_counter >= fail_max:\n",
    "                print(f\"Early stopping triggered in epochs {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1713634206317,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "UwIIWE7p9p4f"
   },
   "outputs": [],
   "source": [
    "# Function of evaluation on validation set\n",
    "\n",
    "def CNN_evaluation(model, data_loader):\n",
    "\n",
    "    # store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    # evaluation on data_loader\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Convert tensors to numpy arrays and append to lists\n",
    "            true_labels.extend(labels.numpy())\n",
    "            pred_labels.extend(predicted.numpy())\n",
    "\n",
    "    # Evaluate performance\n",
    "    accuracy = metrics.accuracy_score(true_labels, pred_labels)\n",
    "    weighted_f1 = metrics.precision_recall_fscore_support(true_labels, pred_labels, average='weighted')[2]\n",
    "    report = metrics.classification_report(true_labels, pred_labels)\n",
    "\n",
    "    # print results\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Weighted F1-score:\", weighted_f1)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39791,
     "status": "ok",
     "timestamp": 1713514275049,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "IZEnuE1U5lY8",
    "outputId": "63b1da43-34cb-42f8-d87b-314e1e908fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered in epochs 10\n",
      "CNN with one conventional layer of 6 filters and kernel size = 3\n",
      "Accuracy: 0.9311064718162839\n",
      "Weighted F1-score: 0.9191890750199524\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85        75\n",
      "           1       0.95      0.98      0.97       388\n",
      "           2       0.33      0.06      0.11        16\n",
      "\n",
      "    accuracy                           0.93       479\n",
      "   macro avg       0.71      0.63      0.64       479\n",
      "weighted avg       0.91      0.93      0.92       479\n",
      "\n",
      "Early stopping triggered in epochs 10\n",
      "CNN with one conventional layer of 6 filters and kernel size = 5\n",
      "Accuracy: 0.9331941544885177\n",
      "Weighted F1-score: 0.9185889437984446\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86        75\n",
      "           1       0.95      0.98      0.97       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.93       479\n",
      "   macro avg       0.60      0.62      0.61       479\n",
      "weighted avg       0.90      0.93      0.92       479\n",
      "\n",
      "Early stopping triggered in epochs 10\n",
      "CNN with one conventional layer of 12 filters and kernel size = 3\n",
      "Accuracy: 0.9394572025052192\n",
      "Weighted F1-score: 0.927700429788484\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87        75\n",
      "           1       0.96      0.98      0.97       388\n",
      "           2       0.50      0.06      0.11        16\n",
      "\n",
      "    accuracy                           0.94       479\n",
      "   macro avg       0.77      0.65      0.65       479\n",
      "weighted avg       0.93      0.94      0.93       479\n",
      "\n",
      "Early stopping triggered in epochs 9\n",
      "CNN with one conventional layer of 12 filters and kernel size = 5\n",
      "Accuracy: 0.9248434237995825\n",
      "Weighted F1-score: 0.909643293706849\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83        75\n",
      "           1       0.94      0.98      0.96       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.92       479\n",
      "   macro avg       0.59      0.60      0.60       479\n",
      "weighted avg       0.90      0.92      0.91       479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNNs model with one convolutional layer\n",
    "CNN_1_6_3 = CNN_model(1, 6, 3, train_loader, monitor_loader)\n",
    "print(\"CNN with one convolutional layer of 6 filters and kernel size = 3\")\n",
    "CNN_evaluation(CNN_1_6_3, val_loader)\n",
    "\n",
    "CNN_1_6_5 = CNN_model(1, 6, 5, train_loader, monitor_loader)\n",
    "print(\"CNN with one convolutional layer of 6 filters and kernel size = 5\")\n",
    "CNN_evaluation(CNN_1_6_5, val_loader)\n",
    "\n",
    "CNN_1_12_3 = CNN_model(1, 12, 3, train_loader, monitor_loader)\n",
    "print(\"CNN with one convolutional layer of 12 filters and kernel size = 3\")\n",
    "CNN_evaluation(CNN_1_12_3, val_loader)\n",
    "\n",
    "CNN_1_12_5 = CNN_model(1, 12, 5, train_loader, monitor_loader)\n",
    "print(\"CNN with one convolutional layer of 12 filters and kernel size = 5\")\n",
    "CNN_evaluation(CNN_1_12_5, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72963,
     "status": "ok",
     "timestamp": 1713514351176,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "DzaUoWSm3IrI",
    "outputId": "3c4fb84d-cf31-40fb-9866-2624a9f817ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered in epochs 19\n",
      "CNN with two conventional layer of 6 filters and kernel size = 3\n",
      "Accuracy: 0.9227557411273486\n",
      "Weighted F1-score: 0.9117918705340889\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82        75\n",
      "           1       0.95      0.98      0.96       388\n",
      "           2       0.25      0.06      0.10        16\n",
      "\n",
      "    accuracy                           0.92       479\n",
      "   macro avg       0.67      0.62      0.63       479\n",
      "weighted avg       0.91      0.92      0.91       479\n",
      "\n",
      "Early stopping triggered in epochs 20\n",
      "CNN with two conventional layer of 6 filters and kernel size = 5\n",
      "Accuracy: 0.941544885177453\n",
      "Weighted F1-score: 0.9376791921637208\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87        75\n",
      "           1       0.97      0.97      0.97       388\n",
      "           2       0.62      0.31      0.42        16\n",
      "\n",
      "    accuracy                           0.94       479\n",
      "   macro avg       0.81      0.73      0.75       479\n",
      "weighted avg       0.94      0.94      0.94       479\n",
      "\n",
      "Early stopping triggered in epochs 11\n",
      "CNN with two conventional layer of 12 filters and kernel size = 3\n",
      "Accuracy: 0.9373695198329853\n",
      "Weighted F1-score: 0.9212942706192638\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87        75\n",
      "           1       0.95      0.99      0.97       388\n",
      "           2       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.94       479\n",
      "   macro avg       0.61      0.62      0.61       479\n",
      "weighted avg       0.91      0.94      0.92       479\n",
      "\n",
      "Early stopping triggered in epochs 12\n",
      "CNN with two conventional layer of 12 filters and kernel size = 5\n",
      "Accuracy: 0.9394572025052192\n",
      "Weighted F1-score: 0.9355155706742979\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86        75\n",
      "           1       0.97      0.98      0.97       388\n",
      "           2       0.56      0.31      0.40        16\n",
      "\n",
      "    accuracy                           0.94       479\n",
      "   macro avg       0.79      0.72      0.74       479\n",
      "weighted avg       0.93      0.94      0.94       479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNNs model with two convolutional layers\n",
    "CNN_2_6_3 = CNN_model(2, 6, 3, train_loader, monitor_loader)\n",
    "print(\"CNN with two convolutional layer of 6 filters and kernel size = 3\")\n",
    "CNN_evaluation(CNN_2_6_3, val_loader)\n",
    "\n",
    "CNN_2_6_5 = CNN_model(2, 6, 5, train_loader, monitor_loader)\n",
    "print(\"CNN with two convolutional layer of 6 filters and kernel size = 5\")\n",
    "CNN_evaluation(CNN_2_6_5, val_loader)\n",
    "\n",
    "CNN_2_12_3 = CNN_model(2, 12, 3, train_loader, monitor_loader)\n",
    "print(\"CNN with two convolutional layer of 12 filters and kernel size = 3\")\n",
    "CNN_evaluation(CNN_2_12_3, val_loader)\n",
    "\n",
    "CNN_2_12_5 = CNN_model(2, 12, 5, train_loader, monitor_loader)\n",
    "print(\"CNN with two convolutional layer of 12 filters and kernel size = 5\")\n",
    "CNN_evaluation(CNN_2_12_5, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo2qY_G5nhz3"
   },
   "source": [
    "With highest weighted F1-score as the selected model in this combination.\n",
    "\n",
    "CNN with with two convolutional layers of 12 filters and kernel size = 5 is choosen. Now store the classifier for the model test part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1713514377884,
     "user": {
      "displayName": "Ling-Yun Huang",
      "userId": "07776248856149240127"
     },
     "user_tz": -60
    },
    "id": "dwFOQ0tB8J9O",
    "outputId": "70c68e95-fac4-44eb-efdd-d474b91d9f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_2(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=384, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "CNN_classifier = CNN_2_6_5\n",
    "\n",
    "# Store trained model\n",
    "torch.save(CNN_classifier, os.path.join(GOOGLE_DRIVE_PATH, 'Models/CNN_classifier.pth'))\n",
    "print(CNN_classifier)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOsNeybHzOw18RzIuGYmOoP",
   "mount_file_id": "1eyy3Z7dnL1JLv9v11ntCnsBkRZH-PLma",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
